{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "from functools import reduce\n",
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from operator import add\n",
    "import shortuuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"com-orkut.top5000.cmty.txt\", 'r') as file:\n",
    "with open(\"testgraph\", 'r') as file:\n",
    "    lines = file.read().split(\"\\n\")\n",
    "\n",
    "print(\"lines\")\n",
    "#print(lines)\n",
    "vertices = []\n",
    "existing_vertices = set()\n",
    "edges = []\n",
    "hyperedges = {}\n",
    "counter=0\n",
    "for l in lines:\n",
    "    if l == \"\":\n",
    "        continue\n",
    "    counter+=1\n",
    "    all_line_vertices = l.split(\" \")\n",
    "    hypid = \"he\" + str(counter)\n",
    "    try:\n",
    "        all_line_vertices.remove(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    hyperedges[hypid] = all_line_vertices\n",
    "    for v in all_line_vertices:\n",
    "        if not v in existing_vertices:\n",
    "            vertices.append((v,\"vertex\"))\n",
    "            existing_vertices.add(v)\n",
    "        if not hypid in existing_vertices:\n",
    "            vertices.append((hypid,\"hyperedge\"))\n",
    "            existing_vertices.add(hypid)\n",
    "        edges.append((v,hypid))\n",
    "        edges.append((hypid,v))\n",
    "\n",
    "vertex_metadata = ['id','type']\n",
    "edges_metadata = ['src','dst']\n",
    "#print(vertices)\n",
    "#print(edges)\n",
    "\n",
    "vertices = spark.createDataFrame(vertices,vertex_metadata)\n",
    "edges = spark.createDataFrame(edges,edges_metadata)\n",
    "\n",
    "\n",
    "\n",
    "print(\"EDGESRCDEST\")\n",
    "print(hyperedges)\n",
    "hyp_deg = spark.sparkContext.broadcast(hyperedges)\n",
    "g = GraphFrame(vertices, edges)## Take a look at the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "| src| dst|\n",
      "+----+----+\n",
      "|   0|hyp0|\n",
      "|hyp0|   0|\n",
      "|   1|hyp0|\n",
      "|hyp0|   1|\n",
      "+----+----+\n",
      "\n",
      "df col None\n",
      "+---+---+---+\n",
      "| _1| _2| id|\n",
      "+---+---+---+\n",
      "|  0|  1|  0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import Row\n",
    "import uuid\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from uuid import UUID\n",
    "\n",
    "def f(x, uuid):\n",
    "    #return 1,x.value.split(\" \")[0]\n",
    "    d = {}\n",
    "    elements = x.value.split(\" \")\n",
    "    for i in range(len(elements)):\n",
    "        yield elements[i],uuid\n",
    "        yield uuid, elements[i]\n",
    "    #for i in range(len(x)):\n",
    "    #    d[\"abc\"] = x[i]\n",
    "    #return d\n",
    "\n",
    "def make_edge_df(edge):\n",
    "    d = {}\n",
    "    d[\"src\"] = edge[0]\n",
    "    d[\"dst\"] = edge[1]\n",
    "    return d\n",
    "\n",
    "def make_vertex_df(edge):\n",
    "    yield edge.src,\"vertex\"\n",
    "    yield edge.dst, \"vertex\"\n",
    "#     try:\n",
    "#         uuid_obj = UUID(edge[0])\n",
    "#         yield edge[0],\"hyperedge\"\n",
    "#     except ValueError:\n",
    "#         yield edge[0], \"vertex\"\n",
    "    \n",
    "#     try:\n",
    "#         uuid_obj = UUID(edge[1])\n",
    "#         yield edge[1],\"hyperedge\"\n",
    "#     except ValueError:\n",
    "#         yield edge[1], \"vertex\"\n",
    "    \n",
    "        \n",
    "def hyp_df(x, delimit):\n",
    "    return x.value.split(delimit)\n",
    "\n",
    "def parse_hyp(x):\n",
    "    hypid = \"hyp\" + str(x[len(x)-1])\n",
    "    for i in range(len(x)-1):\n",
    "        yield x[i],hypid\n",
    "        yield hypid,x[i]\n",
    "\n",
    "#Now populate that\n",
    "#lines = spark.read.text(\"testgraph\").rdd.flatMap(lambda x: f(x,str(uuid.uuid4())))#.toDF()\n",
    "\n",
    "hyp_lines = spark.read.text(\"testgraph\").rdd.map(lambda x: hyp_df(x, \" \")).toDF()\n",
    "hyp_lines = hyp_lines.withColumn(\"id\", monotonically_increasing_id())\n",
    "\n",
    "df_vert = hyp_lines.rdd.flatMap(lambda x: parse_hyp(x)).toDF([\"src\",\"dst\"])\n",
    "print(\"df col\",df_vert.show())\n",
    "\n",
    "#new_test = [list(row) for row in hyp_lines.collect()]\n",
    "#print(new_test)\n",
    "hyp_lines.show()\n",
    "\n",
    "\n",
    "#df_edges = lines.map(lambda x:Row(**make_edge_df(x))).toDF()\n",
    "\n",
    "#df_edges = lines.map(lambda x:(x[0],x[1])).toDF([\"src\",\"dst\"])\n",
    "\n",
    "#df_vertices = df_edges.rdd.map(lambda x:(x.src,x.dst)).toDF([\"id\",\"type\"])\n",
    "#print(df_edges.show())\n",
    "#df_vertices.show()\n",
    "#print(lines.collect())\n",
    "#df = rdd.map(lambda x: Row(**f(x))).toDF()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vertices.filter('id == \"56d21a1f-4342-4d22-a486-3ae4960c9890\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_metadata = ['id','type']\n",
    "edges_metadata = ['src','dst']\n",
    "vertices = []\n",
    "existing_vertices = set()\n",
    "vex = []\n",
    "hyperedges = {}\n",
    "counter=0\n",
    "existing_vertices = set()\n",
    "with open(\"com-orkut.top5000.cmty.txt\", 'r') as file:\n",
    "    lines = file.read().split(\"\\n\")\n",
    "\n",
    "for l in lines:\n",
    "    if l == \"\":\n",
    "        continue\n",
    "    counter+=1\n",
    "    all_line_vertices = l.split(\"\\t\")\n",
    "    hypid = \"he\" + str(counter)\n",
    "    try:\n",
    "        all_line_vertices.remove(\"\")\n",
    "    except:\n",
    "        pass\n",
    "    hyperedges[hypid] = all_line_vertices\n",
    "    for v in all_line_vertices:\n",
    "        if not v in existing_vertices:\n",
    "            try:\n",
    "                frame_vertices = frame_vertices.union(spark.createDataFrame([(v,\"vertex\")],vertex_metadata))\n",
    "            except:\n",
    "                frame_vertices = spark.createDataFrame([(v,\"vertex\")],vertex_metadata)\n",
    "            vex.append((v,\"vertex\"))\n",
    "            existing_vertices.add(v)\n",
    "        if not hypid in existing_vertices:\n",
    "            try:\n",
    "                frame_vertices = frame_vertices.union(spark.createDataFrame([(hypid,\"hyperedge\")],vertex_metadata))\n",
    "            except:\n",
    "                frame_vertices = spark.createDataFrame([(hypid,\"hyperedge\")],vertex_metadata)\n",
    "            vex.append((hypid,\"hyperedge\"))\n",
    "            existing_vertices.add(hypid)\n",
    "        if \"ev\" in locals() or \"ev\" in globals():\n",
    "            ev = ev.union(spark.createDataFrame([(v,hypid)],edges_metadata))\n",
    "            ev = ev.union(spark.createDataFrame([(hypid,v)],edges_metadata))\n",
    "        else:\n",
    "            ev = spark.createDataFrame([(v,hypid)],edges_metadata)\n",
    "            ev = evg.union(spark.createDataFrame([(hypid,v)],edges_metadata))\n",
    "#         edges.append((v,hypid))\n",
    "#         edges.append((hypid,v))\n",
    "\n",
    "edges.show()\n",
    "frame_vertices.show()\n",
    "print(vex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edges.show()\n",
    "g.vertices.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_page_rank(vertex, count):\n",
    "    return 1/count\n",
    "\n",
    "def parseNeighbors(edge):\n",
    "    dest_id = edge.dst\n",
    "    if edge.dst in hyperedges:\n",
    "        return edge.src, hyperedges[edge.dst]\n",
    "    return edge.src,[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContributions(ur):\n",
    "    len_urls = len(ur[1][0])\n",
    "    for site in sum(ur[1][0],[]):\n",
    "        yield site, ur[1][1]/(len_urls-1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_pagerank(iterations, graph):\n",
    "    all_vertices =g.vertices.filter('type==\"vertex\"')\n",
    "    num_vertices = all_vertices.count()\n",
    "    all_edges = graph.edges\n",
    "    ranks = all_vertices.rdd.map(lambda x:(x.id,initialize_page_rank(x, num_vertices))) #\n",
    "    hyperedge_links = all_edges.rdd.map(lambda el: parseNeighbors(el)).groupByKey().cache() # vertex hyp1 hyp2 hyp3\n",
    "    for i in range(iterations):\n",
    "        result_links = hyperedge_links.join(ranks).flatMap(lambda ur:getContributions(ur))\n",
    "        ranks = result_links.reduceByKey(add).mapValues(lambda rank: rank * 0.15 + 0.15)\n",
    "    print(ranks.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_pagerank(5,g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edges.filter('src == \"1\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outgoing(node_id, graph):\n",
    "    df2 = g.edges.filter('src==\"'+node_id+'\"')\n",
    "    df2.show()\n",
    "    df1 = graph.edges\n",
    "    df3 = df1.alias(\"df1\").join(broadcast(df2.alias(\"df2\")), col(\"df2.dst\") == col(\"df1.src\")).select(df1[\"dst\"])\n",
    "    df3.distinct().show() \n",
    "\n",
    "def get_incoming(node_id, graph):\n",
    "    df2 = g.edges.filter('dst==\"'+node_id+'\"')\n",
    "    df2.show()\n",
    "    df1 = graph.edges\n",
    "    df3 = df1.alias(\"df1\").join(broadcast(df2.alias(\"df2\")), col(\"df2.src\") == col(\"df1.dst\")).select(df1[\"src\"])\n",
    "    df3.distinct().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
